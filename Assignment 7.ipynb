{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a04dcd67-4662-4294-8efa-c640b53df1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1544feb5-ee89-429f-87e4-48403000cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c00acc9a-e937-47a5-b9d2-ebc648f0ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Car evaluation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 1: Read the dataset\n",
    "input_file = r\"D:\\Data Science\\car_evaluation.csv\"\n",
    "data = spark.read.csv(input_file, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b84aa278-5279-4cad-825c-3191410dbc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(vhigh0='vhigh', vhigh1='vhigh', 22='2', 23='2', small='small', low='med', unacc='unacc')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "885d26f4-8de9-409f-b570-8b2667dfe9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Assemble feature columns into a feature vector\n",
    "feature_assembler = VectorAssembler(inputCols=[\n",
    "    \"vhigh0_index\",\n",
    "    \"vhigh1_index\",\n",
    "    \"small_index\",\n",
    "    \"low_index\",\n",
    "], outputCol=\"features\")\n",
    "\n",
    "# Step 2: Create a Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"unacc_index\")\n",
    "\n",
    "# Step 3: Create a pipeline with the assembler and logistic regression\n",
    "pipeline = Pipeline(stages=[feature_assembler, lr])\n",
    "\n",
    "# Step 4: Fit the model\n",
    "model = pipeline.fit(new_df)\n",
    "\n",
    "# Step 5: Transform the data to get predictions\n",
    "data_transformed = model.transform(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8014ca79-8b31-46c9-b8fe-785364afdf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6844238563983787\n",
      "F1 Score: 0.6460104226983208\n",
      "Precision: 0.6138909010508511\n",
      "Recall: 0.6844238563983787\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate the model using different metrics\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"unacc_index\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = evaluator.evaluate(data_transformed)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Additional evaluation metrics: F1-score, precision, recall\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"unacc_index\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"unacc_index\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"unacc_index\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "f1_score = f1_evaluator.evaluate(data_transformed)\n",
    "precision = precision_evaluator.evaluate(data_transformed)\n",
    "recall = recall_evaluator.evaluate(data_transformed)\n",
    "\n",
    "# Print additional metrics\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd6b6fa-8b4a-4f95-b4d4-a5cb492fb5aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+---------+-----------+\n",
      "|vhigh0_index|vhigh1_index|small_index|low_index|unacc_index|\n",
      "+------------+------------+-----------+---------+-----------+\n",
      "|         3.0|         3.0|        2.0|      1.0|        0.0|\n",
      "|         3.0|         3.0|        2.0|      0.0|        0.0|\n",
      "|         3.0|         3.0|        1.0|      2.0|        0.0|\n",
      "|         3.0|         3.0|        1.0|      1.0|        0.0|\n",
      "|         3.0|         3.0|        1.0|      0.0|        0.0|\n",
      "|         3.0|         3.0|        0.0|      2.0|        0.0|\n",
      "|         3.0|         3.0|        0.0|      1.0|        0.0|\n",
      "|         3.0|         3.0|        0.0|      0.0|        0.0|\n",
      "|         3.0|         3.0|        2.0|      2.0|        0.0|\n",
      "|         3.0|         3.0|        2.0|      1.0|        0.0|\n",
      "|         3.0|         3.0|        2.0|      0.0|        0.0|\n",
      "|         3.0|         3.0|        1.0|      2.0|        0.0|\n",
      "|         3.0|         3.0|        1.0|      1.0|        0.0|\n",
      "|         3.0|         3.0|        1.0|      0.0|        0.0|\n",
      "|         3.0|         3.0|        0.0|      2.0|        0.0|\n",
      "|         3.0|         3.0|        0.0|      1.0|        0.0|\n",
      "|         3.0|         3.0|        0.0|      0.0|        0.0|\n",
      "|         3.0|         3.0|        2.0|      2.0|        0.0|\n",
      "|         3.0|         3.0|        2.0|      1.0|        0.0|\n",
      "|         3.0|         3.0|        2.0|      0.0|        0.0|\n",
      "+------------+------------+-----------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# List of columns to encode\n",
    "columns_to_encode = ['vhigh0', 'vhigh1', 'small', 'low', 'unacc']\n",
    "\n",
    "# Create a list of StringIndexers for each column\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in columns_to_encode]\n",
    "\n",
    "# Create a pipeline to apply all indexers\n",
    "pipeline_encode = Pipeline(stages=indexers)\n",
    "\n",
    "# Fit and transform the data using the pipeline\n",
    "model = pipeline_encode.fit(data)\n",
    "encoded_data = model.transform(data)\n",
    "\n",
    "# Show the encoded data\n",
    "encoded_data.select([col + \"_index\" for col in columns_to_encode]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a8925f1-c883-45f4-85ef-77313fa53544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+---------+-----------+---+----+\n",
      "|vhigh0_index|vhigh1_index|small_index|low_index|unacc_index| 22|  23|\n",
      "+------------+------------+-----------+---------+-----------+---+----+\n",
      "|         3.0|         3.0|        2.0|      1.0|        0.0|  2|   2|\n",
      "|         3.0|         3.0|        2.0|      0.0|        0.0|  2|   2|\n",
      "|         3.0|         3.0|        1.0|      2.0|        0.0|  2|   2|\n",
      "|         3.0|         3.0|        1.0|      1.0|        0.0|  2|   2|\n",
      "|         3.0|         3.0|        1.0|      0.0|        0.0|  2|   2|\n",
      "|         3.0|         3.0|        0.0|      2.0|        0.0|  2|   2|\n",
      "|         3.0|         3.0|        0.0|      1.0|        0.0|  2|   2|\n",
      "|         3.0|         3.0|        0.0|      0.0|        0.0|  2|   2|\n",
      "|         3.0|         3.0|        2.0|      2.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        2.0|      1.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        2.0|      0.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        1.0|      2.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        1.0|      1.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        1.0|      0.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        0.0|      2.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        0.0|      1.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        0.0|      0.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        2.0|      2.0|        0.0|  2|more|\n",
      "|         3.0|         3.0|        2.0|      1.0|        0.0|  2|more|\n",
      "|         3.0|         3.0|        2.0|      0.0|        0.0|  2|more|\n",
      "+------------+------------+-----------+---------+-----------+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_encode = ['vhigh0', 'vhigh1', 'small', 'low', 'unacc']\n",
    "columns_without_encoding = ['22', '23']\n",
    "final_columns = [col + \"_index\" for col in columns_to_encode] + columns_without_encoding\n",
    "# Create a new DataFrame with the required columns\n",
    "new_df = encoded_data.select(final_columns)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51da97ed-ea99-4047-b1c7-63646d70d538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data:\n",
      "+------------+------------+-----------+---------+-----------+---+----+\n",
      "|vhigh0_index|vhigh1_index|small_index|low_index|unacc_index| 22|  23|\n",
      "+------------+------------+-----------+---------+-----------+---+----+\n",
      "|         3.0|         3.0|        2.0|      1.0|        0.0|  2|   2|\n",
      "|         3.0|         3.0|        2.0|      0.0|        0.0|  2|   2|\n",
      "|         3.0|         3.0|        1.0|      2.0|        0.0|  2|   2|\n",
      "|         3.0|         3.0|        1.0|      1.0|        0.0|  2|   2|\n",
      "|         3.0|         3.0|        1.0|      0.0|        0.0|  2|   2|\n",
      "|         3.0|         3.0|        0.0|      2.0|        0.0|  2|   2|\n",
      "|         3.0|         3.0|        0.0|      1.0|        0.0|  2|   2|\n",
      "|         3.0|         3.0|        0.0|      0.0|        0.0|  2|   2|\n",
      "|         3.0|         3.0|        2.0|      2.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        2.0|      1.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        2.0|      0.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        1.0|      2.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        1.0|      1.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        1.0|      0.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        0.0|      2.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        0.0|      1.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        0.0|      0.0|        0.0|  2|   4|\n",
      "|         3.0|         3.0|        2.0|      2.0|        0.0|  2|more|\n",
      "|         3.0|         3.0|        2.0|      1.0|        0.0|  2|more|\n",
      "|         3.0|         3.0|        2.0|      0.0|        0.0|  2|more|\n",
      "+------------+------------+-----------+---------+-----------+---+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+-----------------+\n",
      "|label|         features|\n",
      "+-----+-----------------+\n",
      "|  0.0|[3.0,3.0,2.0,1.0]|\n",
      "|  0.0|[3.0,3.0,2.0,0.0]|\n",
      "|  0.0|[3.0,3.0,1.0,2.0]|\n",
      "|  0.0|[3.0,3.0,1.0,1.0]|\n",
      "|  0.0|[3.0,3.0,1.0,0.0]|\n",
      "|  0.0|[3.0,3.0,0.0,2.0]|\n",
      "|  0.0|[3.0,3.0,0.0,1.0]|\n",
      "|  0.0|[3.0,3.0,0.0,0.0]|\n",
      "|  0.0|[3.0,3.0,2.0,2.0]|\n",
      "|  0.0|[3.0,3.0,2.0,1.0]|\n",
      "|  0.0|[3.0,3.0,2.0,0.0]|\n",
      "|  0.0|[3.0,3.0,1.0,2.0]|\n",
      "|  0.0|[3.0,3.0,1.0,1.0]|\n",
      "|  0.0|[3.0,3.0,1.0,0.0]|\n",
      "|  0.0|[3.0,3.0,0.0,2.0]|\n",
      "|  0.0|[3.0,3.0,0.0,1.0]|\n",
      "|  0.0|[3.0,3.0,0.0,0.0]|\n",
      "|  0.0|[3.0,3.0,2.0,2.0]|\n",
      "|  0.0|[3.0,3.0,2.0,1.0]|\n",
      "|  0.0|[3.0,3.0,2.0,0.0]|\n",
      "+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "# Show the initial data\n",
    "print(\"Initial Data:\")\n",
    "new_df.show()\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "\n",
    "# Convert the label column to numerical values\n",
    "label_indexer = StringIndexer(inputCol=\"unacc_index\", outputCol=\"label\")  # Ensure \"diagnosis\" is your label column\n",
    "\n",
    "# Drop existing features column if it exists\n",
    "if \"features\" in new_df.columns:\n",
    "    new_df = new_df.drop(\"features\")\n",
    "\n",
    "# Create feature vector\n",
    "feature_assembler = VectorAssembler(inputCols=[\n",
    "    \"vhigh0_index\",\n",
    "    \"vhigh1_index\",\n",
    "    \"small_index\",\n",
    "    \"low_index\"\n",
    "], outputCol=\"features\")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[label_indexer, feature_assembler])\n",
    "\n",
    "# Fit the model (this applies both transformations in the pipeline)\n",
    "model = pipeline.fit(new_df)\n",
    "\n",
    "# Transform the data with the fitted model\n",
    "data_transformed = model.transform(new_df)\n",
    "\n",
    "# Show the transformed data\n",
    "data_transformed.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faa9c304-a463-4eac-8bf2-2b8a20572f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Initialize evaluator for accuracy\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"unacc_index\", metricName=\"accuracy\")\n",
    "\n",
    "# Initialize evaluator for F1 score\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"unacc_index\", metricName=\"f1\")\n",
    "\n",
    "# Compute accuracy and F1 score\n",
    "accuracy = evaluator_accuracy.evaluate(data_transformed)\n",
    "f1_score = evaluator_f1.evaluate(data_transformed)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553acb36-2853-42f8-b75a-130f53ae3721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
