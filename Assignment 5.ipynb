{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44fce8da-0707-4025-9a16-dae11abd084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e95faeef-0d97-4e0c-ab6b-31410e1dfa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c4eff3c-cd72-461a-83bf-94583e9bda16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad46a5f-876c-45e5-81bc-303bd3f065d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName = \"A2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64418e4c-2f39-477f-951e-c8467d8dc4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "+------------------------------------------+\n",
      "|value                                     |\n",
      "+------------------------------------------+\n",
      "|Hello, this is assignment 5.              |\n",
      "|Subject : Big data                        |\n",
      "|This is a new line added to the text file.|\n",
      "+------------------------------------------+\n",
      "\n",
      "Updated Data:\n",
      "Hello, this is assignment 5.\n",
      "Subject : Big data\n",
      "This is a new line added to the text file.\n",
      "This is a new line added to the text file.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import os\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadAppendTextFile\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 1: Define the input file\n",
    "input_file = r\"D:\\Data Science\\abc.txt\"\n",
    "\n",
    "# Step 2: Read existing data from the text file\n",
    "df = spark.read.text(input_file)\n",
    "\n",
    "# Step 3: Show the original data\n",
    "print(\"Original Data:\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Step 4: Add a new line to the DataFrame\n",
    "new_line = \"This is a new line added to the text file.\"  \n",
    "new_row = spark.createDataFrame([Row(value=new_line)])\n",
    "\n",
    "# Combine the original DataFrame with the new row\n",
    "updated_df = df.union(new_row)\n",
    "\n",
    "# Step 5: Collect the updated DataFrame back to Python\n",
    "updated_lines = updated_df.select(\"value\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Step 6: Write the updated data back to the same text file using Python file handling\n",
    "with open(input_file, 'w') as f:\n",
    "    for line in updated_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "# Show the updated data\n",
    "print(\"Updated Data:\")\n",
    "for line in updated_lines:\n",
    "    print(line)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc6cc246-39a1-4abc-bb77-ecefbeddcb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Word Count: 19\n",
      "Unique Word Count: 18\n",
      "Unique Words:\n",
      "+----------+\n",
      "|word      |\n",
      "+----------+\n",
      "|assignment|\n",
      "|new       |\n",
      "|Subject   |\n",
      "|is        |\n",
      "|data      |\n",
      "|5.        |\n",
      "|the       |\n",
      "|:         |\n",
      "|Big       |\n",
      "|This      |\n",
      "|Hello,    |\n",
      "|text      |\n",
      "|a         |\n",
      "|this      |\n",
      "|line      |\n",
      "|added     |\n",
      "|file.     |\n",
      "|to        |\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q2. Design a script in pyspark to count total words in a given doc. Also print unique words in it\n",
    "\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 1: Define the input file\n",
    "input_file = r\"D:\\Data Science\\abc.txt\"\n",
    "\n",
    "# Step 2: Read the text file\n",
    "df = spark.read.text(input_file)\n",
    "\n",
    "# Step 3: Split each line into words and explode to get a word per row\n",
    "words_df = df.select(explode(split(col(\"value\"), \"\\s+\")).alias(\"word\"))\n",
    "\n",
    "# Step 4: Remove any empty strings (in case of extra spaces)\n",
    "words_df = words_df.filter(words_df.word != \"\")\n",
    "\n",
    "# Step 5: Count total words\n",
    "total_word_count = words_df.count()\n",
    "\n",
    "# Step 6: Get unique words\n",
    "unique_words_df = words_df.select(\"word\").distinct()\n",
    "\n",
    "# Step 7: Count unique words (optional, if you need the count)\n",
    "unique_word_count = unique_words_df.count()\n",
    "\n",
    "# Step 8: Show results\n",
    "print(f\"Total Word Count: {total_word_count}\")\n",
    "print(f\"Unique Word Count: {unique_word_count}\")\n",
    "\n",
    "print(\"Unique Words:\")\n",
    "unique_words_df.show(truncate=False)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55457252-d776-4a1b-8ed4-cdf75736e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Create a schema for storing employee details (eno , ename, eage , gender, salary, city). Insert appropriate records. Use streaming to fetch the data for following queries. \n",
    "# Find the details of employee genderwise.\n",
    "# Display  ename and age with respect to salary and city in descending order\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Step 1: Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EmployeeDetails\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Define the schema for employee data\n",
    "schema = StructType([\n",
    "    StructField(\"eno\", IntegerType(), True),\n",
    "    StructField(\"ename\", StringType(), True),\n",
    "    StructField(\"eage\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"salary\", FloatType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Step 3: Simulate streaming data by reading from a folder \n",
    "input_dir = r\"D:\\Data Science\\Deep learning\\employee\\employee.csv\"\n",
    "\n",
    "# Reading the employee data as a streaming source\n",
    "employee_stream = spark.readStream \\\n",
    "    .schema(schema) \\\n",
    "    .csv(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80135325-cb4a-4a90-8fbe-1a6a7a5320b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Perform aggregation: Compute the average salary per city and sort by the average salary in descending order\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m aggregated_data \u001b[38;5;241m=\u001b[39m employee_stream\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg(avg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msalary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_salary\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Sort the aggregated data by average salary (DESC) and city (DESC)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sorted_aggregated_data \u001b[38;5;241m=\u001b[39m aggregated_data\u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_salary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc(), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform aggregation: Compute the average salary per city and sort by the average salary in descending order\n",
    "aggregated_data = employee_stream.groupBy(\"city\").agg(avg(\"salary\").alias(\"avg_salary\"))\n",
    "\n",
    "# Sort the aggregated data by average salary (DESC) and city (DESC)\n",
    "sorted_aggregated_data = aggregated_data.orderBy(col(\"avg_salary\").desc(), col(\"city\").desc())\n",
    "\n",
    "# Output the sorted results to the console\n",
    "sorted_query = sorted_aggregated_data.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the stream to finish\n",
    "sorted_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ce10f74-fafd-4327-b306-ae1e379e6175",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode;\nSort [salary#45 DESC NULLS LAST, city#46 DESC NULLS LAST], true\n+- Project [ename#42, eage#43, salary#45, city#46]\n   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@410a940d,csv,List(),Some(StructType(StructField(eno,IntegerType,true),StructField(ename,StringType,true),StructField(eage,IntegerType,true),StructField(gender,StringType,true),StructField(salary,FloatType,true),StructField(city,StringType,true))),List(),None,Map(path -> D:\\Data Science\\Deep learning\\employee\\employee.csv),None), FileSource[D:\\Data Science\\Deep learning\\employee\\employee.csv], [eno#41, ename#42, eage#43, gender#44, salary#45, city#46]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m\n\u001b[0;32m     10\u001b[0m genderwise_query \u001b[38;5;241m=\u001b[39m genderwise_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Query 2: Sorted ename and eage with respect to salary and city in descending order\u001b[39;00m\n\u001b[0;32m     16\u001b[0m sorted_query \u001b[38;5;241m=\u001b[39m sorted_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Wait for the streaming to finish\u001b[39;00m\n\u001b[0;32m     22\u001b[0m genderwise_query\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\sql\\streaming\\readwriter.py:1527\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[1;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[0;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart())\n\u001b[0;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode;\nSort [salary#45 DESC NULLS LAST, city#46 DESC NULLS LAST], true\n+- Project [ename#42, eage#43, salary#45, city#46]\n   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@410a940d,csv,List(),Some(StructType(StructField(eno,IntegerType,true),StructField(ename,StringType,true),StructField(eage,IntegerType,true),StructField(gender,StringType,true),StructField(salary,FloatType,true),StructField(city,StringType,true))),List(),None,Map(path -> D:\\Data Science\\Deep learning\\employee\\employee.csv),None), FileSource[D:\\Data Science\\Deep learning\\employee\\employee.csv], [eno#41, ename#42, eage#43, gender#44, salary#45, city#46]\n"
     ]
    }
   ],
   "source": [
    "# Query 1: Find the details of employees genderwise\n",
    "genderwise_df = employee_stream.groupBy(\"gender\").count()\n",
    "\n",
    "# Query 2: Display ename and age with respect to salary and city in descending order\n",
    "sorted_df = employee_stream.select(\"ename\", \"eage\", \"salary\", \"city\") \\\n",
    "    .orderBy(col(\"salary\").desc(), col(\"city\").desc())\n",
    "\n",
    "# Step 4: Output the results to the console\n",
    "# Query 1: Gender-wise employee details\n",
    "genderwise_query = genderwise_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Query 2: Sorted ename and eage with respect to salary and city in descending order\n",
    "sorted_query = sorted_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the streaming to finish\n",
    "genderwise_query.awaitTermination()\n",
    "sorted_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a5ec2-c854-41c6-bcd8-f326c5537433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
